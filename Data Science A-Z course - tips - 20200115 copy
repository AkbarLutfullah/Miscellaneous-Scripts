ETL notes

Editing dates - data>text to columns>delimiter uncheck>set to date>MDY format
              - right click and edit to YYYYMMDD format
			  
Editing balances - create new column 
                 - type in code: =SUBSTITUTE(SUBSTITUTE($A1,"$",""),",","")
				
Avoiding errors with data

- Once you have created a dataflow from the flat file to OLE DB and you are looking to input your csv file into flat file be careful about the text qualifer
- If you do not specify a tq then your data will get skewed i.e. in this case the card type had numbers etc
- To avoid this tq needs to be "
- Note : if you do not specify a text qualifier then SSIS no longer recognises quotations as text or a string and therefore seprates the feedback cell by the comma
- so in essence because we want to segregate our data from one speech bubble to the next, we use " as a text qualifier and SSIS understands the column starts before the " and also ends after it
- for every project have separate database or separate schema

Fixing anomalies with text
- stack up original text file with prepared file in notepadd++
- go to row that is throwing an error or is skewed 
- check to see if " are lined up, excel compensates for corrupt data by including " which in turn produces unwanted " and this affects the data entry into the flat file
- place " in the correct location and proceed
- if the original is showing missing data then you may need to remove the data altogether as even excel cannot fix that
- so both initial errors have been around text qualifier ". In one case it was in the wrong place and in the other case the data had missing columns so we got rid off it

when you incur error The column "Column x" cannot be processed because more than one code page (65001 and 1252) are specified for it...THERE are 2 solutions:
1. Set the "alwaysUseDefaultCodePage" property of the destination OLEDB connection to True.
2. Change the codepage property of the OLEDB connection to 65001 or flat file connection to 1252-ANSI
3. Once we executed the data, the analysis stopped at row 31k - we check the error message and go to that row in notepadd++..turns out there was a truncation error as the "feedback" column had an extremely long message
4. To fix this, we go back into the flat file connection manager and change the character lentgh from 1k to 5k, the same needs to be done for the flat file source
5. Now delete the table in OLE DB and recreate new table
6. View in SQL studio to validate results

Finding corrupt errors/anomalies in SQL
- so once you have uploaded all the rows in your dataset, select all rows in SQL via table command
- you may notice discrepancies in rows between the total uploaded and the total displayed i.e. our dataset contains 50k rows, the number displayed by SQL is 50,004
- we now need to query the data
- tips: query the end of the table i.e. the last few columns 
- we have longitude and lastly column 46 (which is blank)
- 1st query code: WHERE [Column 46] NOT LIKE '' ---> wherever the last column is not blank i.e. a dataset has shifted to the right plz show me this row
- OR [Longitude] NOT LIKE '%.%' ---> wherever the penultimate column does not have a decimal i.e. dataset has shifted to the left plz show me this row
- Basically, we are accoutning of shifts in the data to the right/left
- Once executed we can view the corrupted/missing data rows; upto us if we want to fix these errors in SQL or SSIS, instrcutor prefers SSIS
- for SSIS process, first 'truncate table' in SQL to hide all data other than columns; intermediary step and not entirely sure why it was done, I am now.. so we are doing all this wrangling in SSIS to obtain a nice clean table, our previous table has insufficient data etc which is why we truncate it after which we create conditional splits and then later we can re-view the table for analysis
- go back to SSIS and include additional step between OLE DB destination and flat file source
- move conditional split from sources to other transforms 
- extend blue arrow to conditional split and add in condition of right/left shifts under 'bad records'; at the bottom include 'good records' as the opposite of bad record commands
- add flat file source destination as we need to save our bad records somewhere
- create new folder in analysis section under bad records - once you have executed your data, this folder will be automatically populated with bad records
- extend other blue line to OLE DB destination and this will automatically be named good records
- NOTE - you may have to switch up code page default language(s) to 65001/1252 - ANSI

Essential check once the data has been uploaded to the database
- Open SQL and delete everything from select - from
- Type COUNT(*) 
- This number of rows must equal total number of datasets uploaded to OLE DB database 
- Very important to disable your package once the required table has been created and uploaded to the database 

SQL tips and tricks 
- SELECT * FROM [TABLE} : selects all rows
- We already know how to select top 1000 rows
- Can also select specific columns by specifying 
- Think of dbo as a prefix/subfolder/schema
- To avoid selecting DB in top left hand drop down and in the command after from, just type in the following at the top of the script: USE 'DB' enter GO
- If you have entered data into SQL as text then when you want to bring up the numbers from a specific column such as furniture, you will have to pass the FLOAT command before the column
- To identify/search for elements in a string be sure to make use of wildcards; you will have to use LIKE/NOT LIKE and % and _
- Code for multiline comments */ and end with /*
- Single line comment -- and end with nothing 
- Order table by 'product name', type in 'ORDER BY' after FROM.. this will give ascending order i.e A-Z 
- For descending add DESC at the end
- To arrange sales (numericals) by order, you cant just pass the above command as we inputted data into SQL as text
- Instead you need to pass the following ORDER BY CAST ([Sales] as FLOAT) DESC



Data Types
- Varchar(n) - basically any string or text and n is # of characters; 'Hello, my name is Akbar'
- Int - 25 - refrain from using int cos even if order id or region are in numericals, the fact that we arent going to be taking averages of these indexes or muliplying them etc we cant really treat them as numbers, instead they are varchar
- Float - 104.40 [if you multiply int by float, resultng column will be float]
- Date - yyyy-mm-dd
- Datetime - yyyy-mm-dd hh:mm:ss
- Bit - 1 
- Money - run FROM IT, always convert to float type in SQL
- SQL converts data using implicit and explicit conversions
- Implicit ones are where SQL automatically does it
- Explicit ones are where we use CAST/CONVERT
- When ordering by numericals and imported as text file; ORDER BY (CAST[Sales] as FLOAT) same as ORDER BY CONVERT(FLOAT, [Sales])... for dates use convert cos after second comma you can specify data, for all else use CAST
- When comparing to NULLs, keep in mind normal operators the ones that would work on integers and floats won't apply i.e. you cant use <>,= etc
- Instead you can say IS NULL NOT NULL i.e. [discount] is NULL 

Joining tables in SQL
- Inner joins - only common rows are obtained
- Left joins - all rows in left table obtained + common rows
- Right joins - vice versa
- Full join - contains all rows from both tables + common elements 
- Use multiple fields when joining to avoid duplication of results
- code for left join:
SELECT * 
FROM [TABLE A]A
LEFT JOIN [TABLE B] B
ON A.[ORDER ID] = B.[ORDER ID]

- Stored procedures in DS used to save our code in SQL for codes between BEGIN and END
- VERY IMPORTANT TO RUN THE WHOLE PROC BEFORE CLOSIGN SQL COS OTHERWISE THE PROC WONT BE SAVED AND UPDATED
- BE SURE TO HIGHLIGHT PROC TO SAVE IT and change template so that ALTER PROC comes right after GO and before author
- To check a result just highlight the command and execute, but to save you will have to hihlight the from alter proc to end OR the whole script
- Keep zipcodes in VARCHAR cos this data type is not going to be modified or ordered etc, so not stored as a number..same with customer id
- 